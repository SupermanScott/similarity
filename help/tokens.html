<!-- $Id: $ -->
Tokens are the hard pieces of data that is used by the similarity
algorithm. They represent the properties of the Drupal entities that
are the basis for the calculation. A token has a relevancy score
associated with it. This score represents how 'relevent' the token is
to its object. The most common calculation for this relevancy is
called Term Frequency / Inverse Document Frequency. This concept is
roughly equivlent to <em>How many times is this token used in the object
divided by the number of times this token is seen throughout the
site</em>.

<dl>
	<dt><a href="&topic:similarity/tokenizers&">Tokenizers</a></dt>
	<dd>A Tokenizer takes the raw data from the database and calculates
	the relevancy score</dd>
	<dt><a href="&topic:similarity/transformers&">Transformers</a></dt>
	<dd>A Transformer takes the tokens generated by the Tokenizers and
	transforms them into new tokens. Common techniques are to remove
	tokens and boost relevancy for different tokens.
</dl>